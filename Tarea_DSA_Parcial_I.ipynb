{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuanMa216/Estructura_De_Datos/blob/main/Tarea_DSA_Parcial_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enunciado**  \n",
        "1. Considere un lugar para almacenar los parámetros de la red. Dado que cada neurona debe tener asociados unos pesos y un sesgo (*bias*), necesitamos almacenarlos de manera eficiente para que los algoritmos de *backpropagation* y evaluación puedan acceder a ellos. Proponga una adaptación de una o varias estructuras de datos que permitan realizar esto de la mejor manera. Defina la complejidad de las operaciones en las que incurrirían estos dos algoritmos según sus necesidades.  \n",
        "---\n",
        "## **Solución**  \n",
        "\n",
        "Considerando la necesidad de almacenar los parámetros de una red neuronal de manera eficiente para su acceso en los algoritmos de *backpropagation* y evaluación, la estructura de datos propuesta es el uso de vectores, específicamente una matriz representada como un **vector de vectores**.  \n",
        "\n",
        "Las redes neuronales estudiadas en clase y en la presentación del invitado no presentan cambios en su tamaño ni en el número de neuronas durante la ejecución. Por lo tanto, una estructura de datos adecuada, considerando estas características, es el uso de **vectores**. Estos han sido utilizados en tareas anteriores e implementados en clase, donde se ha observado que su manejo es más cómodo y eficiente, siempre que no se requiera cambiar su tamaño dinámicamente (pues en ese caso, sería necesario copiar el vector existente para expandirlo).  \n",
        "\n",
        "El problema plantea la necesidad de una estructura eficiente tanto en **acceso como en almacenamiento**. Las **matrices de vectores** permiten un acceso directo a cualquier elemento en tiempo **constante \\( O(1) \\)**, lo cual es ideal en este caso para los algoritmos de *backpropagation* y evaluación. Dado que los valores de pesos y *biases* se acceden frecuentemente, no es necesario recorrer la matriz para encontrar un valor (como sí ocurre en listas enlazadas u otras estructuras dinámicas).  \n",
        "\n",
        "Además, el uso de **álgebra matricial y lineal** optimiza las operaciones necesarias, ya que las matrices almacenan sus elementos en memoria **contigua**, mejorando la eficiencia de acceso y aprovechando la caché del procesador. Según investigaciones, esto es **crucial** en redes neuronales, ya que las operaciones matemáticas deben ejecutarse rápidamente sobre grandes volúmenes de datos. Usar **vectores evita el *overhead* del manejo de punteros**, lo que maximiza el rendimiento en memoria.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Complejidad de Forward y Backpropagation**  \n",
        "\n",
        "| **Paso**                  | **Operación**                              | **Complejidad por Capa**       | **Complejidad Total (Red de \\( L \\) capas)** |\n",
        "|--------------------------|--------------------------------------|--------------------------|--------------------------------|\n",
        "| **Forward Propagation**  | Multiplicación  $$ W^{(l)} A^{(l-1)} $$  | $$ O(n_l \\cdot n_{l-1}) $$ | $$ O(L \\cdot n^2) $$         |\n",
        "|                          | Suma del *bias* $$ b^{(l)} $$            | $$ O(n_l) $$               | $$ O(L \\cdot n)$$            |\n",
        "|                          | Aplicación de activación $$ f(Z) $$    | $$ O(n_l) $$               | $$ O(L \\cdot n) $$            |\n",
        "| **Backpropagation**      | Cálculo del error en la última capa   | $$ O(n_L) $$               | $$ O(n) $$                    |\n",
        "|                          | Propagación del error hacia atrás     | $$ O(n_l \\cdot n_{l-1}) $$ | $$ O(L \\cdot n^2) $$          |\n",
        "|                          | Actualización de pesos $$ W $$        | $$ O(n_l \\cdot n_{l-1}) $$ | $$ O(L \\cdot n^2) $$          |\n",
        "\n",
        "---\n",
        "\n",
        "En resumen, la estructura de datos propuesta es el **uso de vectores (matrices)**, ya que permiten un acceso **rápido y eficiente** a los elementos gracias a su almacenamiento **contiguo en memoria**. Esto optimiza el rendimiento en redes neuronales que utilizan un **tamaño fijo**, evitando la sobrecarga de estructuras dinámicas.  \n",
        "\n",
        "Si bien las operaciones necesarias para cada algoritmo pueden ser **costosas computacionalmente**, la complejidad asintótica para ambos (**Forward Propagation y Backpropagation**) es de:  \n",
        "\n",
        "$$\n",
        "O(L \\cdot n^2)\n",
        "$$\n",
        "\n",
        "donde **\\( L \\)** representa el número de capas de la red neuronal.  \n"
      ],
      "metadata": {
        "id": "2LWcRHfcWtlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Enunciado**\n",
        "2. Por ultimo necesitamos representar la red en cada una de sus capas. Por facilidad les propongo el caso de estudio de una red neuronal que reconoce digitos (MNIST). Piense que nuestra red tiene una capa de entrada de 784 neuronas y una capa de salida de 10 neuronas. Piense ademas que tiene un número *h* de capas ocultas densamente conectadas(es decir, cada neurona de la capa *i - 1* tiene una conexión con cada neurona de la capa *i*). Proponga una implementación de la clase *NeuralNetwork* que permita representar esta red. Defina muy bien sus atributos y las operaciones que considere básicas para su evaluación.\n",
        "3. Proponga una implementación de la función *Forward* que permita evaluar la red desde su capa de entrada hasta su capa de salida.\n",
        "---\n",
        "##**Solución**\n",
        "\n",
        "## Clase Neurona\n",
        "### Atributos:\n",
        "- Bias  \n",
        "- Vector de Pesos  \n",
        "- Salida  \n",
        "- Delta (para backpropagation)  \n",
        "\n",
        "## Clase Capa\n",
        "### Atributos:\n",
        "- Vector de Neuronas  \n",
        "\n",
        "## Clase NeuralNetwork\n",
        "### Atributos:\n",
        "- Vector de Capas (cada capa contiene varias neuronas)  \n",
        "- Tasa de aprendizaje  \n",
        "\n",
        "### Métodos:\n",
        "#### `inicializar(numero_capas_ocultas, tamaño_capa_oculta)`\n",
        "- Crear la **capa de entrada** con **784 neuronas**.  \n",
        "- Crear **capas ocultas** con el número y tamaño especificado.  \n",
        "- Crear la **capa de salida** con **10 neuronas**.  \n",
        "- Asignar **pesos aleatorios** a cada conexión entre neuronas.  \n",
        "- Asignar un **bias aleatorio** a cada neurona.  \n",
        "\n",
        "#### `evaluar(entrada)`\n",
        "- Para cada **capa en la red**:  \n",
        "  - Para cada **neurona en la capa**:  \n",
        "    - Calcular **suma ponderada** de entradas + bias.  \n",
        "    - Aplicar **función de activación** (sigmoide).  \n",
        "    - Guardar **salida de la neurona**.  \n",
        "- Retornar **salida final**.  \n",
        "\n",
        "#### `entrenar(entrada, salida_esperada)`\n",
        "1. Evaluar la red con la entrada para obtener **salida real**.  \n",
        "2. Calcular **error en la capa de salida**:  \n",
        "   - `error = salida_esperada - salida_real`  \n",
        "   - `delta_salida = error * derivada_sigmoide(salida_real)`  \n",
        "3. Retropropagar error a capas ocultas:  \n",
        "   - Para cada **capa desde la salida hasta la entrada**:  \n",
        "     - Para cada **neurona en la capa**:  \n",
        "       - Calcular **error basado en los pesos** de la siguiente capa.  \n",
        "       - `delta = error * derivada_sigmoide(salida_neurona)`.  \n",
        "4. Ajustar **pesos y bias**:  \n",
        "   - Para cada **neurona en cada capa**:  \n",
        "     - Para cada **peso**:  \n",
        "       - `peso += tasa_aprendizaje * delta * salida_anterior`  \n",
        "     - `bias += tasa_aprendizaje * delta`  \n",
        "\n",
        "#### `derivada_sigmoide(x)`\n",
        "- Retornar `x * (1 - x)`.  \n",
        "\n",
        "#### `entrenar_red(datos_entrenamiento, etiquetas, epocas)`\n",
        "- Para cada **época**:  \n",
        "  - Para cada **muestra en los datos de entrenamiento**:  \n",
        "    - Llamar a `entrenar` con la muestra y su etiqueta.  \n",
        "\n",
        "#### `predecir(entrada)`\n",
        "- Llamar a `evaluar` (forward) con la entrada.  \n",
        "- Retornar la **neurona con el mayor valor de salida** (clase predicha).  \n",
        "\n",
        "### Main\n",
        "- Crear una **instancia de la red neuronal** con parámetros adecuados.  \n",
        "- Generar **datos de entrenamiento** y **etiquetas**.  \n",
        "- Llamar a `entrenar_red` con los datos y número de épocas.  \n",
        "- Evaluar la red con **nuevas entradas**.  \n",
        "- **Mostrar resultados**.  \n",
        "\n"
      ],
      "metadata": {
        "id": "g9p8cNbK6AMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cpp\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cmath>\n",
        "#include <random>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "class NeuralNetwork\n",
        "{\n",
        "private:\n",
        "    // Clase interna que representa una neurona\n",
        "    class Neuron\n",
        "    {\n",
        "    public:\n",
        "        double bias;            // Valor de sesgo de la neurona\n",
        "        vector<double> weights; // Lista de pesos de las conexiones de entrada\n",
        "        double output;          // Salida de la neurona tras la activación\n",
        "        double delta;           // Delta para el ajuste en backpropagation\n",
        "\n",
        "        // Constructor: inicializa pesos y bias aleatorios\n",
        "        Neuron(int num_inputs)\n",
        "        {\n",
        "            random_device rd;\n",
        "            mt19937 gen(rd());\n",
        "            uniform_real_distribution<double> dist(-1.0, 1.0);\n",
        "            bias = dist(gen);\n",
        "            weights.resize(num_inputs); // Reserva espacio en el vector weights para almacenar num_inputs elementos. (Los inicializa con 0.0)\n",
        "            for (double &w : weights)   // &w: Referencia a cada elemento dentro del vector weights.\n",
        "            {\n",
        "                w = dist(gen);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Calcula la salida de la neurona aplicando la función de activación (sigmoide)\n",
        "        double activate(const vector<double> &inputs)\n",
        "        {\n",
        "            double sum = bias;\n",
        "            for (unsigned int i = 0; i < inputs.size(); i++)\n",
        "            {\n",
        "                sum += inputs[i] * weights[i];\n",
        "            }\n",
        "            output = sigmoid(sum); // Aplicar sigmoide a la suma ponderada\n",
        "            return output;\n",
        "        }\n",
        "\n",
        "        // Ajusta los pesos y el bias en base al error calculado y la tasa de aprendizaje\n",
        "        void updateWeights(double learning_rate, const vector<double> &inputs)\n",
        "        {\n",
        "            for (unsigned int i = 0; i < weights.size(); i++)\n",
        "            {\n",
        "                weights[i] += learning_rate * delta * inputs[i]; // Ajuste de pesos\n",
        "            }\n",
        "            bias += learning_rate * delta; // Ajuste del bias\n",
        "        }\n",
        "\n",
        "    private:\n",
        "        // Función de activación sigmoide\n",
        "        static double sigmoid(double x)\n",
        "        {\n",
        "            return 1.0 / (1.0 + exp(-x));\n",
        "        }\n",
        "    };\n",
        "\n",
        "    // Clase interna que representa una capa de la red neuronal\n",
        "    class Layer\n",
        "    {\n",
        "    public:\n",
        "        vector<Neuron> neurons; // Vector de neuronas en la capa\n",
        "\n",
        "        // Constructor: Crea una capa con un número dado de neuronas\n",
        "        Layer(int num_neurons, int num_inputs)\n",
        "        {\n",
        "            for (int i = 0; i < num_neurons; i++)\n",
        "            {\n",
        "                neurons.push_back(Neuron(num_inputs));\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Realiza la propagación hacia adelante de la capa\n",
        "        vector<double> forwardPass(const vector<double> &inputs)\n",
        "        {\n",
        "            vector<double> outputs;\n",
        "            for (Neuron &neuron : neurons)\n",
        "            {\n",
        "                outputs.push_back(neuron.activate(inputs));\n",
        "            }\n",
        "            return outputs;\n",
        "        }\n",
        "    };\n",
        "    // Atributos\n",
        "    vector<Layer> layers; // Vector de capas de la red neuronal\n",
        "    double learning_rate; // Tasa de aprendizaje\n",
        "\n",
        "    // Función de activación sigmoide\n",
        "    static double sigmoid(double x)\n",
        "    {\n",
        "        return 1.0 / (1.0 + exp(-x));\n",
        "    }\n",
        "\n",
        "    // Derivada de la función sigmoide\n",
        "    static double sigmoidDerivative(double x)\n",
        "    {\n",
        "        return x * (1.0 - x);\n",
        "    }\n",
        "\n",
        "public:\n",
        "    // Constructor: Inicializa la red neuronal con capas ocultas y tasa de aprendizaje\n",
        "    NeuralNetwork(int hidden_layers, int hidden_neurons, double rate)\n",
        "    {\n",
        "        learning_rate = rate;\n",
        "        // Capa oculta inicial con conexiones desde la entrada (784)\n",
        "        layers.emplace_back(hidden_neurons, 784);\n",
        "\n",
        "        // Capas ocultas intermedias\n",
        "        for (int i = 1; i < hidden_layers; i++)\n",
        "        {\n",
        "            layers.emplace_back(hidden_neurons, hidden_neurons);\n",
        "            //layers.push_back(Layer(hidden_neurons, hidden_neurons));\n",
        "        }\n",
        "\n",
        "        // Capa de salida con 10 neuronas y conexiones desde la última capa oculta\n",
        "        layers.emplace_back(10, hidden_neurons);\n",
        "    }\n",
        "\n",
        "    // Propagación hacia adelante: calcula la salida de la red neuronal\n",
        "    vector<double> forward(const vector<double> &input)\n",
        "    {\n",
        "        vector<double> activations = input;\n",
        "        for (Layer &layer : layers)\n",
        "        {\n",
        "            activations = layer.forwardPass(activations);\n",
        "        }\n",
        "        return activations;\n",
        "    }\n",
        "\n",
        "    // Entrenamiento de la red neuronal usando backpropagation\n",
        "    void train(const vector<double> &input, const vector<double> &expected_output)\n",
        "    {\n",
        "        // Paso 1: Propagación hacia adelante\n",
        "        vector<double> output = forward(input);\n",
        "\n",
        "        // Paso 2: Calcular el error y los deltas en la capa de salida\n",
        "        for (unsigned int i = 0; i < layers.back().neurons.size(); i++)\n",
        "        {\n",
        "            double error = expected_output[i] - layers.back().neurons[i].output;\n",
        "            layers.back().neurons[i].delta = error * sigmoidDerivative(layers.back().neurons[i].output);\n",
        "        }\n",
        "\n",
        "        // Paso 3: Retropropagación del error en capas ocultas\n",
        "        for (int i = layers.size() - 2; i >= 0; i--)\n",
        "        {\n",
        "            for (unsigned int j = 0; j < layers[i].neurons.size(); j++)\n",
        "            {\n",
        "                double error_sum = 0.0;\n",
        "                for (unsigned int k = 0; k < layers[i + 1].neurons.size(); k++)\n",
        "                {\n",
        "                    error_sum += layers[i + 1].neurons[k].weights[j] * layers[i + 1].neurons[k].delta;\n",
        "                }\n",
        "                layers[i].neurons[j].delta = error_sum * sigmoidDerivative(layers[i].neurons[j].output);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        // Paso 4: Actualizar pesos y bias en la primera capa\n",
        "        for (unsigned int i = 0; i < layers[0].neurons.size(); i++)\n",
        "        {\n",
        "            layers[0].neurons[i].updateWeights(learning_rate, input);\n",
        "        }\n",
        "\n",
        "        // Paso 5: Actualizar pesos y bias en las capas ocultas y de salida\n",
        "        for (unsigned int i = 1; i < layers.size(); i++)\n",
        "        {\n",
        "            for (unsigned int j = 0; j < layers[i].neurons.size(); j++)\n",
        "            {\n",
        "                vector<double> previousOutputs;\n",
        "                for (const Neuron &neuron : layers[i - 1].neurons)\n",
        "                {\n",
        "                    previousOutputs.push_back(neuron.output);\n",
        "                }\n",
        "                layers[i].neurons[j].updateWeights(learning_rate, previousOutputs);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "};\n",
        "\n",
        "int main()\n",
        "{\n",
        "    // Crear una red neuronal con 45 capas ocultas de 100 neuronas y tasa de aprendizaje de 0.03\n",
        "    NeuralNetwork nn(45, 100, 0.03);\n",
        "\n",
        "    // Crear un vector de entrada de tamaño 784 con valores de 0.5\n",
        "    vector<double> input(784, 0.5);\n",
        "\n",
        "    // Crear una salida esperada donde solo el índice 3 es 1.0 (etiqueta de clase 3)\n",
        "    vector<double> expected_output(10, 0.0);\n",
        "    expected_output[5] = 1.0;\n",
        "\n",
        "    // Entrenar la red neuronal con la misma entrada 1000 veces\n",
        "    for (int i = 0; i < 1000; i++)\n",
        "    {\n",
        "        nn.train(input, expected_output);\n",
        "    }\n",
        "\n",
        "    // Evaluar la red neuronal con la misma entrada después del entrenamiento\n",
        "    vector<double> output = nn.forward(input);\n",
        "\n",
        "    // Imprimir la salida de la red neuronal\n",
        "    cout << \"Salida de la red: \";\n",
        "    for (double val : output)\n",
        "    {\n",
        "        cout << val << \" \";\n",
        "    }\n",
        "    cout << endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "WI4lvWUvrF_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f4d688-e586-4b2b-8342-86170e381156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cpp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!g++ main.cpp -o main\n",
        "!./main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I22U3wq2SdNB",
        "outputId": "d386128e-a6a6-44fc-e904-a92d30eac888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Salida de la red: 0.0167087 0.0218052 0.0153408 0.0155954 0.0173472 0.984673 0.0145977 0.0146769 0.014641 0.0155058 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qM19tlt3UNQJ"
      }
    }
  ]
}